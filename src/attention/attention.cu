
#include <stdio.h>
#include <cuda.h>
#include <cuda_fp16.h>
#include "cuda_runtime.h"
#include <cmath>




//ä¸€èˆ¬æ¥è¯´è¾“å…¥çš„æ˜¯å¤šå¤´æ³¨æ„åŠ›
//n_head*(seq_len)*(dims//n_head)

#define N_HEAD 1
#define N 64
#define D 32
#define NUMS N*D*N_HEAD
#define nbytes NUMS*sizeof(float)


/*
Q K V

Q N*D
K N*D
V N*D

*/

//bræ˜¯Q   ,  ä¸€å°å—çš„è¡Œæ•°
//bcæ˜¯K V ï¼Œ ä¸€å°å—çš„è¡Œæ•°

// size_qolm = br
// size_kv   = bc

//å‡½æ•°è°ƒç”¨å…³ç³»
//flash_attention<32,32,128><<<n_head,32>>>
template<int size_kv,int size_qolm,int dims>
__global__ void flash_attention_gpu(
    const float* q,const float* k,const float* v,float* o,
    const int n_head,const int n,const int d,float scale,
    float *l,float *m,
    const int br,const int bc,
    const int tr,const int tc
)
{
    printf("1111111111111111");
    int tid=threadIdx.x;
    int block_index=blockIdx.x;
    const int nums=n*d;

    //æ¯ä¸ªblock,ç‹¬ç«‹çš„è®¡ç®—ä¸€ä¸ªheadï¼Œå¯¹äºæ¯ä¸€ä¸ªblockï¼Œç¡®å®šä¸€ä¸‹æ•°æ®æŒ‡é’ˆçš„æŒ‡å‘
    const float *q_head=q+nums*block_index;
    const float *k_head=k+nums*block_index;
    const float *v_head=v+nums*block_index;
    float *o_head=o+nums*block_index;

    const float *l_head=l+n*block_index;
    const float *m_head=m+n*block_index;


    __shared__ float k_share[size_kv*dims];
    __shared__ float v_share[size_kv*dims];

    __shared__ float q_share[size_qolm*dims];
    __shared__ float o_share[size_qolm*dims];

    //__shared__ float l_share[size_qolm];
    //__shared__ float m_share[size_qolm];

    //__shared__ float s_share[size_kv*size_qolm];
    //Load K ğ‘— and V ğ‘— from HBM to on-chip SRAM.
    /*
        å…±äº«å†…å­˜è¦å¤šå¤§ï¼Ÿ 2*(bc*d) ä¸ª float  bcæ˜¯è¡Œæ•°ï¼Œdæ˜¯åˆ—æ•°ï¼Œ2çš„è¯æ˜¯K+V
    */
    printf("tr=%d ",tr);
    for(int i=0;i<tr;i++)
    {
        


        for(int x=0;x<dims;x++){
            q_share[tid*dims+x]=q_head[i*size_qolm*dims+tid*dims+x];
            o_share[tid*dims+x]=o_head[i*size_qolm*dims+tid*dims+x];
        }
        __syncthreads();
        
        if(tid==0){
            for(int i=0;i<32;i++){
                for(int j=0;j<64;j++){
                    printf("%f ",q[i*64+j]);
                }
                printf("\n");
            }
        }

        //l_share[tid]=l_head[i*size_qolm+tid];
        //m_share[tid]=m_head[i*size_qolm+tid];
        //__syncthreads();


        float m=-100000.f;
        float d=0.0f;

        for(int j=0;j<tc;j++){


            //åŠ è½½KVåˆ°share memory
            //ä»k_headå’Œv_headï¼ŒåŠ è½½size_kvè¡Œæ•°æ®ï¼Œæ¯è¡Œæ•°æ®æœ‰dims
            //1ä¸ªwarpï¼Œ32ä¸ªçº¿ç¨‹ï¼Œæ¯ä¸ªçº¿ç¨‹è¯»å–ä¸€è¡Œçš„æ•°æ®åˆ° share memory
            for(int x=0;x<dims;x++){
                k_share[tid*dims+x]=k_head[j*size_kv*dims+tid*dims+x];
                v_share[tid*dims+x]=v_head[j*size_kv*dims+tid*dims+x];
            }
            __syncthreads();

            

            //è®¡ç®—çŸ©é˜µä¹˜æ³•s=Q*Kt,size_qolm*dims @ size_kv*dims  =size_qolm*size_kv
            //ç›´æ¥å¯¹åº”è¡Œç›¸ä¹˜å°±è¡Œï¼Œæ¯ä¸€ä¸ªçº¿ç¨‹è®¡ç®—s_shareé‡Œçš„,æ¯ä¸€è¡Œ
            
            //s_share[size_qolm*size_kv];
            for(int x=0;x<size_kv;x++){
                float sum=0.0f;
                for(int y=0;y<dims;y++){
                    sum+=q_share[tid*dims+y]*k_share[x*dims+y];
                }
                
                float m_pre=m;
                float d_pre=d;

                m=fmax(m,sum);
                d=d*exp(m_pre-m)+exp(sum-m);

                for(int x=0;x<dims;x++){
                    v_share[tid*dims+x]=o_share[tid*dims+x]*d_pre*exp(m_pre-m)/d+v_share[tid*dims+x]*exp(sum-m)/d;
                }
                __syncthreads();
                //s_share[tid*size_kv+x]=sum;
            }

            
        }

        for(int x=0;x<dims;x++){
            o_head[i*size_qolm*dims+tid*dims+x]=o_share[tid*dims+x];
        }
    }



}



void soft_max(float* data,int cols,int rows)
{   
    for(int i=0;i<rows;i++){
        float max_num=-1000.0f;
        for(int j=0;j<cols;j++){
            max_num=max(max_num,data[i*cols+j]);
        }

        float sum_num=0.0f;
        for(int j=0;j<cols;j++){
            sum_num += exp(data[i*cols+j]-max_num);
        }

        for(int j=0;j<cols;j++){
            data[i*cols+j]= exp(data[i*cols+j]-max_num)/sum_num;
        }

    }

}


//



//
void attention_cpu(float* q,float* k,float* v,float* o,int n_head,int n,int d,float scale)
{
    //printf("scale is %f \n",scale);
    //ç¬¬ä¸€æ­¥ï¼Œè®¡ç®—Q*Kï¼Œæ—¶é—´å¤æ‚åº¦æ˜¯d*n*n
    float* temp =(float*)malloc(n*n*sizeof(float));

    for(int i=0;i<n;i++){
        for(int j=0;j<n;j++){
            float sum=0.0f;
            for(int x=0;x<d;x++){
                sum+=q[i*d+x]*k[j*d+x]; //å› ä¸ºè¦âœ–kçš„è½¬ç½®ï¼Œè¿™é‡Œç›´æ¥è¯»å–è¡Œå‘é‡å³å¯sum+=q[i*d+k]*k[j*d+k
            }
            temp[i*n+j]=sum*scale;
        }
    }

    // for(int i=0;i<n;i++){
    //     for(int j=0;j<n;j++){
    //         printf("%f ",temp[i*n+j]);
    //     }
    //     printf("\n");
    // }


    //ç¬¬äºŒæ­¥ï¼Œå¯¹tempçš„æ¯ä¸€è¡Œåšsoftmax æ—¶é—´å¤æ‚çš„n*n
    soft_max(temp,n,n);




    //ç¬¬ä¸‰éƒ¨ï¼Œè®¡ç®—temp*Vï¼Œæ—¶é—´å¤æ‚åº¦æ˜¯d*n*n
    for(int i=0;i<n;i++){
        for(int j=0;j<d;j++){
            float sum=0.0f;

            for(int x=0;x<n;x++){
                sum+=temp[i*n+x]*v[x*d+j];
            }
            o[i*d+j]=sum;
        }
    }

    free(temp);

}


void attention_cpu_multi_head(float* q,float* k,float* v,float* o,int n_head,int n,int d,float scale)
{
    for(int i=0;i<n_head;i++){

        float* q_head=q+(n*d)*i;
        float* k_head=k+(n*d)*i;
        float* v_head=v+(n*d)*i;

        float* o_head=o+(n*d)*i;

        attention_cpu(q_head,k_head,v_head,o_head,n_head,n,d,scale);

    }
}

bool check_result(float* result,float* groudtruth)
{
    for(int i=0;i<NUMS;i++){
        //printf("result[%d]=%f,groudtruth[%d]=%f \n",i,result[i],i,groudtruth[i]);
        if(abs(result[i]-groudtruth[i])>1e-6){
            printf("result[%d]=%f,groudtruth[%d]=%f \n",i,result[i],i,groudtruth[i]);
            return false;
        }
    }

    return true;
}


void test_attention()
{
    float* q=(float*)malloc(nbytes);
    float* k=(float*)malloc(nbytes);
    float* v=(float*)malloc(nbytes);

    float* o=(float*)malloc(nbytes);

    //init array
    for(int i=0;i<NUMS;i++){
        q[i]=k[i]=v[i]=(float)(i%5);
    }

    float scale=(float)(1/sqrt(D));
    attention_cpu_multi_head(q,k,v,o,N_HEAD,N,D,scale);
    




    float* device_q=nullptr;
    float* device_k=nullptr;
    float* device_v=nullptr;
    float* device_o=nullptr;

    float* device_l=nullptr;
    float* device_m=nullptr;

    cudaMalloc((void **)&device_q, nbytes);
    cudaMalloc((void **)&device_k, nbytes);
    cudaMalloc((void **)&device_v, nbytes);
    cudaMalloc((void **)&device_o, nbytes);

    cudaMalloc((void **)&device_l, nbytes);
    cudaMalloc((void **)&device_m, nbytes);


    cudaMemcpy(device_q, q, nbytes, cudaMemcpyHostToDevice);
    cudaMemcpy(device_k, k, nbytes, cudaMemcpyHostToDevice);
    cudaMemcpy(device_v, v, nbytes, cudaMemcpyHostToDevice);

    float milliseconds = 0;
    cudaEvent_t start, stop;
    cudaEventCreate(&start);
    cudaEventCreate(&stop);
    cudaEventRecord(start);
    flash_attention_gpu<32,32,D><<<N_HEAD,32>>>(device_q,device_k,device_v,device_o,N_HEAD,N,D,scale,device_l,device_m,32,32,N/32,N/32);
    cudaError_t error = cudaGetLastError();
    printf("CUDA error: %s\n", cudaGetErrorString(error));

    cudaEventRecord(stop);
    cudaEventSynchronize(stop);
    cudaEventElapsedTime(&milliseconds, start, stop);


    float* result=(float*)malloc(nbytes);
    cudaMemcpy(result, device_o,nbytes , cudaMemcpyDeviceToHost);

    // for(int i=0;i<NUMS;i++){
    //     //printf("11111111");
    //     printf("%f ",result[i]);
    // }
    if(!check_result(result,o)) printf("result is fail \n");
    printf("layernorm_simple latency = %f ms\n", milliseconds);

}



int main(){



    test_attention();
    return 0;
}
